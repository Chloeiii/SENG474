{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Title(TBD)]\n",
    "\n",
    "Abdulla Almahmood()  \n",
    "Max Gunton()  \n",
    "Yaxi Yu(V00828218)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**1. Data Collection. (TODO)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2. Data Preprocessing.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing process aims to clean and transform the raw data to an useful form. In\n",
    "order to do that, we follow the next steps:  \n",
    "2.1. Deal with missing values  \n",
    "2.2. Data normalization  \n",
    "2.3. Data visualization  \n",
    "2.4. Training and Testing data set generation: To generate the files required for the\n",
    "data mining process (i.e, ARFF files for the WEKA tool)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Structured Data/epi_r.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.1. Deal With Missing Values.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at how we can identify and mark values as missing.  \n",
    "We can use plots and summary statistics to help identify missing or corrupt data.  \n",
    "We can load the dataset as a Pandas DataFrame and print summary statistics on each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we chain a .sum() method on the dataframe, we can see which column contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the column names which contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest strategy for handling missing data is to remove records that contain a missing value.  \n",
    "We can do this by creating a new Pandas DataFrame with the rows containing missing values removed.  \n",
    "Pandas provides the dropna() function that can be used to drop either columns or rows with missing data. We can use dropna() to remove all rows with missing data, as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again to see if we still have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.2. Data normalization.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, the 1st column stores the name of each recipe;  \n",
    "the column 2 to 6 store the generational info of each recipe;  \n",
    "and the rest store the ingredients respectively (the value of these columns are eigher 0 or 1, showing whether each ingredient exist in this recipe or not).   \n",
    "\n",
    "We want to normalize the data in the 2nd to the 6th columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_info = data[['rating', 'calories', 'protein', 'fat', 'sodium']]\n",
    "data_general_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a method to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return (df - df.min()) * 1.0 / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this method to normalze these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_info_normalized = data_general_info.apply(normalize)\n",
    "data_general_info_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.3. Data Visualization.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use seaborn (a Python data visualization library based on matplotlib) to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = data_general_info_normalized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A violin plot is similar to a box plot with a rotated kernel density plot on each side.  \n",
    "Use Violin Plot to visualise the distribution of the data and its probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=data_general_info_normalized)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.4. Training and Testing data set generation. (TODO)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**3. Data Mining.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below takes in the predictions as an array and the correct classifications and uses the two to return the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidenceInterval.py\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# TODO: expand evaluator to handle continuous class labels\n",
    "\n",
    "# class_labels must be discrete and not continuous and numerical\n",
    "\n",
    "class confidenceInterval:\n",
    "    CLT_PROB_DIST = {0.1 : 3.09,    # 99.8% confidence\n",
    "                 0.5 : 2.58,     # 99%     \"\n",
    "                 1.0 : 2.33,    # 98%      \"\n",
    "                 5.0 : 1.65,    # 90%      \"\n",
    "                 10.0 : 1.28,   # 80%      \"\n",
    "                 20.0 : 0.84,   # 60%      \"\n",
    "                 40.0 : 0.25}   # 20%      \"\n",
    "    MAX_SPREAD = 100\n",
    "    p = None\n",
    "    q = None\n",
    "    variance = None\n",
    "    SN = None\n",
    "    N = None\n",
    "    confidence = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, predictions=None, class_labels=None):\n",
    "        pass\n",
    "\n",
    "    def quantizeConfidence(self, confidence):\n",
    "        index = (100 - confidence)/2.0\n",
    "        values = np.array([0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 40.0])\n",
    "        diffs = abs(values - index)\n",
    "        index = values[np.argmin(diffs)]\n",
    "        return (100 - (2*index)) , index\n",
    "\n",
    "    \n",
    "    def establish(self, predictions, class_labels, confidence=90):\n",
    "        \n",
    "        assert (len(predictions) == len(class_labels))\n",
    "\n",
    "        self.confidence, index = self.quantizeConfidence(confidence)\n",
    "        self.N = len(predictions)\n",
    "\n",
    "        # ensure that the input arrays are of type numpy.array\n",
    "        predictions = np.array(predictions)     # predictions.shape = (N,)\n",
    "        class_labels = np.array(class_labels)   # class_labels.shape = (N,)\n",
    "\n",
    "        # check to make sure they are the same shape\n",
    "        assert (predictions.shape == class_labels.shape)\n",
    "\n",
    "        # compare each array element wise to see if they are the same\n",
    "        # given multiple class values if they are represented using integers\n",
    "        # the predictions can be subtracted from the actual class_labels and\n",
    "        # resulting zeros will indicate a successful predictions; anything else will\n",
    "        # indicate an error or incorrect prediction\n",
    "\n",
    "        results = class_labels - predictions\n",
    "        #print(results)  # prints correctly\n",
    "        #print(self.N)  # prints 32\n",
    "        \n",
    "        self.p = float(len([result for result in results if result == 0]))/float(self.N)\n",
    "        self.q = 1-self.p\n",
    "        self.variance = self.p*self.q/self.N\n",
    "        denom = sqrt(self.variance)\n",
    "\n",
    "        self.SN = [(self.p - self.CLT_PROB_DIST[index]*denom),\n",
    "                   (self.p + self.CLT_PROB_DIST[index]*denom)]\n",
    "\n",
    "        return\n",
    "        \n",
    "   \n",
    "    def printConfidence(self):\n",
    "        if self.SN != None:\n",
    "            # \\U+03F5 unicode for within the set\n",
    "            print('Successes: {}\\nErrors: {}\\n___________\\nTotal Instances: {}\\n\\nP: {}\\nQ: {}\\nVariance:'\n",
    "                  ' {}\\n'.format(self.p*self.N, self.q*self.N, self.N, self.p, self.q, self.variance))\n",
    "            print('With {}% confidence probability of correct classification is in the '\n",
    "                  'range {:.1f}% - {:.1f}%\\n\\n'.format(self.confidence,self.SN[0]*100,\n",
    "                                                   self.SN[1]*100))\n",
    "        else:\n",
    "            print('Confidence not yet established\\n\\n')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of a cross validator.  Will add a more in depth description about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidator.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\n",
    "#from confidenceInterval import confidenceInterval\n",
    "\n",
    "# Linear Classifiers\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "# Decision Tree Classifiers\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "CLASS_LABEL = 'rating'\n",
    "NUM_CLASSES = 6\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "df = pd.read_csv('Structured Data/epi_r.csv')  # read in entire dataset\n",
    "df.fillna(-1, inplace=True)    # replace NaN values with -1\n",
    "\n",
    "# split data into attributes and class labels\n",
    "y = df[CLASS_LABEL]\n",
    "X = df[[col for col in df.columns if col != CLASS_LABEL]]\n",
    "\n",
    "# convert y to a numpy array\n",
    "y = np.array(y)\n",
    "y.reshape((-1,))\n",
    "\n",
    "# change y from being continuous values between 0-5 to simply being an integer number\n",
    "# 0, 1, 2, 3, 4, or 5\n",
    "y = pd.cut(y,NUM_CLASSES, right=False, labels=[i for i in range(NUM_CLASSES)])\n",
    "\n",
    "# print(y)\n",
    "# print(X.columns[1])\n",
    "strat_k_fold = StratifiedKFold(n_splits=NUM_SPLITS)  # parameters n_folds=10, shuffle=False, random_state=None\n",
    "\n",
    "# Here we want to cast X and y as numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# want to find the class that occurs the least often and only take that many instances\n",
    "# from each class value so that we end up with an even distribution\n",
    "minimum = np.min([len([val for val in y if val == i]) for i in range(6)])\n",
    "\n",
    "classes = np.array([],dtype=np.int32).reshape(-1,)\n",
    "for i in range(NUM_CLASSES):\n",
    "    classes = np.append(classes,np.array((np.where(y == i)),dtype=np.int32).reshape(-1,)[:minimum])\n",
    "\n",
    "X = X[classes] # extract our subset from entire dataset\n",
    "y = y[classes] #              \"\n",
    "\n",
    "X = X[:, 1:]  # remove the title column from the dataset since these are unique to each entry anyway\n",
    "\n",
    "splits = strat_k_fold.split(X, y)\n",
    "#scores = np.array([], dtype=np.float32).reshape(-1,)\n",
    "for train_index, test_index in splits:\n",
    "    # print('Train Index: {}'.format(train_index))\n",
    "    # print('Test Index: {}'.format(test_index))\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # TODO: use a grid search to determine the best learning constant\n",
    "    #       tweek C and max iterations to get good convergence and speed mix\n",
    "    log_reg_m = LogisticRegression(solver='lbfgs', C=0.1, max_iter=50000, multi_class='multinomial') #max_iter=50000\n",
    "    lin_reg_m = LinearRegression()\n",
    "    dtr_m = DecisionTreeRegressor()\n",
    "    rfr_m = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "    log_reg_m.fit(X_train, y_train)\n",
    "    lin_reg_m.fit(X_train, y_train)\n",
    "    dtr_m.fit(X_train, y_train)\n",
    "    rfr_m.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # print(log_reg_m.score(X_train,y_train))  # validating on test data\n",
    "\n",
    "    y1 = log_reg_m.predict(X_test) # this one works well not to sure about the others\n",
    "    y2 = lin_reg_m.predict(X_test)\n",
    "    y3 = dtr_m.predict(X_test)\n",
    "    y4 = rfr_m.predict(X_test)\n",
    "\n",
    "    # try taking an average and choosing the closest\n",
    "    prediction = (y1 + y2 + y3 + y4) / 4\n",
    "    prediction = np.around(prediction)\n",
    "    c1 = confidenceInterval()\n",
    "    c1.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    c1.printConfidence()\n",
    "\n",
    "    # TODO: try voting based on majority\n",
    "    # prediction = np.column_stack((y1,y2,y3,y4))\n",
    "\n",
    "#print(scores.sum()/strat_k_fold.n_splits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
