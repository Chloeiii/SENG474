{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h1 style=\"text-align:center; color:rgb(153, 99, 0)\">EXPLORING PATTERNS AND TRENDS<br> IN EVERYDAY RECIPES</h1>\n",
    "<h2 style=\"text-align:center; color:rgb(153, 99, 0)\">SENG 474 – DATA MINING</h2>\n",
    "<br>\n",
    "<p style=\"text-align:center; font-size:14pt; font-style:italics\">November 30, 2018</p>\n",
    "<br>\n",
    "\n",
    "<div style=\"height: auto; width: 200px; text-align:right\">\n",
    "<h5 style=\"text-align: left\"><b>Prepared For:</b></h5>\n",
    "Alex Thomo<br>\n",
    "<h5 style=\"text-align: left\"><b>By:</b></h5>\n",
    "Abdulla Almahmood()<br>\n",
    "Max Gunton (V00511318)<br>\n",
    "Yaxi Yu(V00828218)<br>\n",
    "</div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.4em;text-decoration:underline;color:orange\">**Introduction**</span>\n",
    "<p>As long as there has been life on earth there has been food, and no other animal has quite mastered the art of combining and preparing food quite as much as humans.  </p>\n",
    "<p>Scientists generally agree that our early ancestors, Homo erectus, first appeared in Africa 1–2 million years ago. They spread throughout the world and evolved into ancient humans, and approximately 250,000 years ago: hearths appeared.  This is by most accounts the accepted archeological estimate for invention of cooking.  </p>\n",
    "<p>Knowing that cooking and development of recipes has been on going for 250,000 years we figured there must be hidden patterns yet to be discovered in the ingredients and the cooking techniques used.  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:1.4em; text-decoration:underline\">**1.0 Data Collection**</span>\n",
    "<h4 style=\"color:orange\">You are only as good as your data!  And our data was …. Ok.  </h4>\n",
    "<p>Initially we had hoped to collect and process our own data, but we were all quite busy this semester.  Therefore, we decided that in order to get the most out of applying the algorithms and techniques we learned, we would simply use an already existing dataset.  </p>\n",
    "<p>The dataset we chose to use was uploaded to Kaggle.com and can be found using the following link:</p>\n",
    "\t&nbsp;<a href=\"https://www.kaggle.com/hugodarwood/epirecipes#full_format_recipes.json\">https://www.kaggle.com/hugodarwood/epirecipes#full_format_recipes.json</a>\n",
    "<p>The data comes in the form of a 26.7 CSV file and is composed of 20052 rows and 680 columns.  Each row represents a recipe structured as follows:</p>\n",
    "<p>title | rating | calories | protein\tfat\tsodium | … | \"characteristics & ingredients\" </p>\n",
    "\n",
    "<p>Where the first 6 row provide information about each recipe and the following 674 rows contain a 0 or a 1 depending on whether they contain the corresponding ingredient or satisfy the characteristic.  </p>\n",
    "\n",
    "<h3 style=\"color:orange\">1.1 Pitfalls of Our Dataset</h3>\n",
    "\n",
    "<h4 style=\"color:orange\">Overly Simplified</h4>\n",
    "<p>One of the major downsides of using this provided dataset is that you can’t force anything out of it that isn’t already there.  For example, the presence of an ingredient is a good start when it comes to classifying recipes, but it is only part of the equation.  And we would be foolish to believe that the ratios of ingredients (their normalized weights) don’t also play a major part in finding patterns.  If we were collecting the data ourselves, we would have liked to include the weights of the ingredients instead of simply a 1 or 0.  </p>\n",
    "\n",
    "\n",
    "<h4 style=\"color:orange\">Flavour Molecule</h4>\n",
    "<p>Given more time, we would have also liked to have added information about the composition of the ingredients themselves.  More specifically, the composition of flavour molecules that make up each ingredient.  We believe that this is the direction that would result in the most interesting results.  Using these we could compute the Cosine similarity between ingredients which would allow us to offer recommendation for food pairings.  As well as discover which flavor molecules go well with others.  However, gathering the data and structuring it proved to be too time consuming for the scope of this project.  It would also have added a level of complexity that may have been more than we could handle as beginners to data mining.  </p>\n",
    "\n",
    "<h3 style=\"color:orange\">1.2 Benefits of Our Dataset</h3>\n",
    "<p>Some of the positive things about the dataset we are using is that it contains many attributes/columns, and this means that there are many potential relationships to be discovered.  In addition, the data came ready to go for frequent item set analysis, which allowed us to focus on implementing and optimizing the algorithm, rather than playing around with the data.  </p>\n",
    "<p>Our dataset is also large, and this meant that we had to keep in mind efficiency as well as implement things in such a way that they could be generalized.  It also gave us a taste of how things go in industry, when you can’t see all your data at once.  This really drove the importance of using the visualization tools in the matplotlib library.  And although we were able to load our entire dataset into excel, it was on the upper limit of what excel could handle.  This meant that any data preprocessing had to be done using Python, and in a generalized way as it wasn’t feasible to do it by hand.  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:1.4em; text-decoration:underline\">**2.0 Data Preprocessing**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing process aims to clean and transform the raw data to an useful form. In\n",
    "order to do that, we follow the next steps:  \n",
    "2.1. Deal with missing values  \n",
    "2.2. Data normalization  \n",
    "2.3. Data visualization  \n",
    "2.4. Training and Testing data set generation: To generate the files required for the\n",
    "data mining process (i.e, ARFF files for the WEKA tool)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Structured Data/epi_r.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.1. Deal With Missing Values.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at how we can identify and mark values as missing.  \n",
    "We can use plots and summary statistics to help identify missing or corrupt data.  \n",
    "We can load the dataset as a Pandas DataFrame and print summary statistics on each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we chain a .sum() method on the dataframe, we can see which column contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the column names which contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest strategy for handling missing data is to remove records that contain a missing value.  \n",
    "We can do this by creating a new Pandas DataFrame with the rows containing missing values removed.  \n",
    "Pandas provides the dropna() function that can be used to drop either columns or rows with missing data. We can use dropna() to remove all rows with missing data, as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again to see if we still have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.2. Data normalization.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, the 1st column stores the name of each recipe;  \n",
    "the column 2 to 6 store the generational info of each recipe;  \n",
    "and the rest store the ingredients respectively (the value of these columns are eigher 0 or 1, showing whether each ingredient exist in this recipe or not).   \n",
    "\n",
    "We want to normalize the data in the 2nd to the 6th columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_info = data[['rating', 'calories', 'protein', 'fat', 'sodium']]\n",
    "data_general_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a method to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return (df - df.min()) * 1.0 / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this method to normalze these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_general_info_normalized = data_general_info.apply(normalize)\n",
    "data_general_info_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.3. Data Visualization.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use seaborn (a Python data visualization library based on matplotlib) to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = data_general_info_normalized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A violin plot is similar to a box plot with a rotated kernel density plot on each side.  \n",
    "Use Violin Plot to visualise the distribution of the data and its probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=data_general_info_normalized)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Global attribute names:  \n",
    "    **data** ... full dataset without missing values  \n",
    "    **data_general_info** ... 2nd to 6th columns without missing values  \n",
    "    **data_general_info_normalized**  ... normalized 2nd to 6th columns without missing values\n",
    "</span>  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;\">**2.4 Training and Testing data set generation**</span>  \n",
    "We decided to implement the training / testing split using K-Fold cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidator.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\n",
    "#from confidenceInterval import confidenceInterval\n",
    "\n",
    "# Linear Classifiers\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "# Decision Tree Classifiers\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "CLASS_LABEL = 'rating'\n",
    "NUM_CLASSES = 6\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "#df = pd.read_csv('Structured Data/epi_r.csv')  # read in entire dataset\n",
    "#df.fillna(-1, inplace=True)    # replace NaN values with -1\n",
    "df = data\n",
    "\n",
    "# split data into attributes and class labels\n",
    "y = df[CLASS_LABEL]\n",
    "X = df[[col for col in df.columns if col != CLASS_LABEL]]\n",
    "\n",
    "# convert y to a numpy array\n",
    "y = np.array(y)\n",
    "y.reshape((-1,))\n",
    "\n",
    "# change y from being continuous values between 0-5 to simply being an integer number\n",
    "# 0, 1, 2, 3, 4, or 5\n",
    "y = pd.cut(y,NUM_CLASSES, right=False, labels=[i for i in range(NUM_CLASSES)])\n",
    "\n",
    "# print(y)\n",
    "# print(X.columns[1])\n",
    "strat_k_fold = StratifiedKFold(n_splits=NUM_SPLITS)  # parameters n_folds=10, shuffle=False, random_state=None\n",
    "\n",
    "# Here we want to cast X and y as numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# want to find the class that occurs the least often and only take that many instances\n",
    "# from each class value so that we end up with an even distribution\n",
    "minimum = np.min([len([val for val in y if val == i]) for i in range(6)])\n",
    "\n",
    "classes = np.array([],dtype=np.int32).reshape(-1,)\n",
    "for i in range(NUM_CLASSES):\n",
    "    classes = np.append(classes,np.array((np.where(y == i)),dtype=np.int32).reshape(-1,)[:minimum])\n",
    "\n",
    "X = X[classes] # extract our subset from entire dataset\n",
    "y = y[classes] #              \"\n",
    "\n",
    "X = X[:, 1:]  # remove the title column from the dataset since these are unique to each entry anyway\n",
    "\n",
    "splits = strat_k_fold.split(X, y)\n",
    "#scores = np.array([], dtype=np.float32).reshape(-1,)\n",
    "count = 1\n",
    "for train_index, test_index in splits:\n",
    "    # print('Train Index: {}'.format(train_index))\n",
    "    # print('Test Index: {}'.format(test_index))\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # TODO: use a grid search to determine the best learning constant\n",
    "    #       tweek C and max iterations to get good convergence and speed mix\n",
    "    log_reg_m = LogisticRegression(solver='lbfgs', C=0.1, max_iter=50000, multi_class='multinomial') #max_iter=50000\n",
    "    lin_reg_m = LinearRegression()\n",
    "    dtr_m = DecisionTreeRegressor()\n",
    "    rfr_m = RandomForestRegressor(n_estimators=100)\n",
    "    nb_m = BernoulliNB()\n",
    "\n",
    "    log_reg_m.fit(X_train, y_train)\n",
    "    lin_reg_m.fit(X_train, y_train)\n",
    "    dtr_m.fit(X_train, y_train)\n",
    "    rfr_m.fit(X_train, y_train)\n",
    "    nb_m.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # print(log_reg_m.score(X_train,y_train))  # validating on test data\n",
    "\n",
    "    y1 = log_reg_m.predict(X_test) # this one works well not to sure about the others\n",
    "    y2 = lin_reg_m.predict(X_test)\n",
    "    y3 = dtr_m.predict(X_test)\n",
    "    y4 = rfr_m.predict(X_test)\n",
    "    y5 = nb_m.predict(X_test)\n",
    "\n",
    "    # try taking an average and choosing the closest\n",
    "    #prediction = (y1 + y2 + y3 + y4 + y5) / 5\n",
    "    #prediction = np.around(prediction)\n",
    "    prediction = y1\n",
    "    c1 = confidenceInterval()\n",
    "    c1.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print('\\nFOLD -  {}\\n'.format(count))\n",
    "    c1.printConfidence()\n",
    "    count += 1\n",
    "\n",
    "    # TODO: try voting based on majority\n",
    "    # prediction = np.column_stack((y1,y2,y3,y4))\n",
    "\n",
    "#print(scores.sum()/strat_k_fold.n_splits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:1.4em; text-decoration:underline\">**3. Data Mining**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. This project is aiming to find interesting patterns and relationships between vairous attributes and do predictions on the ratings of each recipe. \n",
    "\n",
    "To achieve this, we first tried to use WEKA to classify the attributes using various machine learning classifiers. However, we had a hart time importing our large csv file to Weka.\n",
    "\n",
    "We first tried to import the csv file directly to WEKA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka1.png\" style=\"height:250px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, an error occured saying \"attribute names are not unique!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka2.png\" style=\"height:100px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 680 columns in total, finding the duplicates and rename them would be time consuming. Therefore, we tried to simply remove the titles, and add a range of numbers to place them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka3.png\" style=\"height:200px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately, a different error occured when we tried to import this file again, saying \"680 Problem encountered on line: 23\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka4.png\" style=\"height:100px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While trying to find the resolvent online, we found that there's a number of programmers having problems loading large size csv to WEKA. Either the file itself is not accepted, or the loading and processing process take too much of their time.  \n",
    "Therefore, we come into aggreement to code the classifers ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below takes in the predictions as an array and the correct classifications and uses the two to return the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidenceInterval.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# TODO: expand evaluator to handle continuous class labels\n",
    "\n",
    "# class_labels must be discrete and not continuous and numerical\n",
    "\n",
    "class confidenceInterval:\n",
    "    CLT_PROB_DIST = {0.1 : 3.09,    # 99.8% confidence\n",
    "                 0.5 : 2.58,     # 99%     \"\n",
    "                 1.0 : 2.33,    # 98%      \"\n",
    "                 5.0 : 1.65,    # 90%      \"\n",
    "                 10.0 : 1.28,   # 80%      \"\n",
    "                 20.0 : 0.84,   # 60%      \"\n",
    "                 40.0 : 0.25}   # 20%      \"\n",
    "    MAX_SPREAD = 100\n",
    "    p = None\n",
    "    q = None\n",
    "    variance = None\n",
    "    SN = None\n",
    "    N = None\n",
    "    confidence = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, predictions=None, class_labels=None):\n",
    "        pass\n",
    "\n",
    "    def quantizeConfidence(self, confidence):\n",
    "        index = (100 - confidence)/2.0\n",
    "        values = np.array([0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 40.0])\n",
    "        diffs = abs(values - index)\n",
    "        index = values[np.argmin(diffs)]\n",
    "        return (100 - (2*index)) , index\n",
    "\n",
    "    \n",
    "    def establish(self, predictions, class_labels, confidence=90):\n",
    "        \n",
    "        assert (len(predictions) == len(class_labels))\n",
    "\n",
    "        self.confidence, index = self.quantizeConfidence(confidence)\n",
    "        self.N = len(predictions)\n",
    "\n",
    "        # ensure that the input arrays are of type numpy.array\n",
    "        predictions = np.array(predictions)     # predictions.shape = (N,)\n",
    "        class_labels = np.array(class_labels)   # class_labels.shape = (N,)\n",
    "\n",
    "        # check to make sure they are the same shape\n",
    "        assert (predictions.shape == class_labels.shape)\n",
    "\n",
    "        # compare each array element wise to see if they are the same\n",
    "        # given multiple class values if they are represented using integers\n",
    "        # the predictions can be subtracted from the actual class_labels and\n",
    "        # resulting zeros will indicate a successful predictions; anything else will\n",
    "        # indicate an error or incorrect prediction\n",
    "\n",
    "        results = class_labels - predictions\n",
    "        #print(results)  # prints correctly\n",
    "        #print(self.N)  # prints 32\n",
    "        \n",
    "        self.p = float(len([result for result in results if result == 0]))/float(self.N)\n",
    "        self.q = 1-self.p\n",
    "        self.variance = self.p*self.q/self.N\n",
    "        denom = np.sqrt(self.variance)\n",
    "\n",
    "        self.SN = [(self.p - self.CLT_PROB_DIST[index]*denom),\n",
    "                   (self.p + self.CLT_PROB_DIST[index]*denom)]\n",
    "\n",
    "        return\n",
    "        \n",
    "   \n",
    "    def printConfidence(self):\n",
    "        if self.SN != None:\n",
    "            # \\U+03F5 unicode for within the set\n",
    "            print('Successes: {}\\nErrors: {}\\n___________\\nTotal Instances: {}\\n\\nP: {}\\nQ: {}\\nVariance:'\n",
    "                  ' {}\\n'.format(self.p*self.N, self.q*self.N, self.N, self.p, self.q, self.variance))\n",
    "            print('With {}% confidence probability of correct classification is in the '\n",
    "                  'range {:.1f}% - {:.1f}%\\n\\n'.format(self.confidence,self.SN[0]*100,\n",
    "                                                   self.SN[1]*100))\n",
    "        else:\n",
    "            print('Confidence not yet established\\n\\n')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of a cross validator.  Will add a more in depth description about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_curve.py\n",
    "\n",
    "import cv2\n",
    "\n",
    "class ROC_curve:\n",
    "\n",
    "    SCALE = 4000\n",
    "    y = None\n",
    "    predictions = None\n",
    "\n",
    "    def __init__(self, predictions, y):\n",
    "        self.y = (np.array(y, dtype=np.int32)).reshape((-1,))\n",
    "        self.predictions = (np.array(predictions, dtype=np.int32)).reshape((-1,))\n",
    "        assert (y.shape == predictions.shape)\n",
    "\n",
    "    def compare_labels(self, axis):\n",
    "        # 0 - TP\n",
    "        # 1 - TN\n",
    "        # 2 - FP\n",
    "        # 3 - FN\n",
    "        diff = axis[0] - axis[1]\n",
    "        if diff == 0:\n",
    "            if axis[0] == 1:\n",
    "                # True Positive\n",
    "                return 0\n",
    "            else:\n",
    "                # True Negative\n",
    "                return 1\n",
    "        else:\n",
    "            if axis[0] == 0:\n",
    "                # False Positive\n",
    "                return 2\n",
    "            else:\n",
    "                # False Negative\n",
    "                return 3\n",
    "\n",
    "    def calcPoint(self):\n",
    "\n",
    "        # y = y.reshape((-1,))\n",
    "        # predictions = predictions.reshape((-1,))\n",
    "        values = np.column_stack((self.y, self.predictions))\n",
    "\n",
    "\n",
    "        # print(y)\n",
    "        # print(predictions)\n",
    "\n",
    "        a = np.apply_along_axis(self.compare_labels,1,values)\n",
    "        labels, counts = np.unique(a, return_counts=True)\n",
    "        labels2, counts2 = np.unique(self.y, return_counts=True)\n",
    "        # print(a)\n",
    "        # print(counts)\n",
    "        # print(counts2)\n",
    "        index = dict(zip(labels, counts))\n",
    "        index2 = dict(zip(labels2, counts2))\n",
    "        print(index2)\n",
    "        confusion_matrix = {'TruePos' : index.get(0,0),\n",
    "                            'TrueNeg' : index.get(1,0),\n",
    "                            'FalsePos' : index.get(2,0),\n",
    "                            'FalseNeg' : index.get(3,0),\n",
    "                            'Positives' : index2.get(1,0),\n",
    "                            'Negatives' : index2.get(0,0)\n",
    "                            }\n",
    "\n",
    "        true_positive_rate = confusion_matrix['TruePos'] / confusion_matrix['Positives']\n",
    "        false_positive_rate = confusion_matrix['FalsePos'] / confusion_matrix['Negatives']\n",
    "\n",
    "        graph_point = np.array([np.round(false_positive_rate,2)*self.SCALE,np.round(true_positive_rate,2)*self.SCALE], dtype=np.int32)\n",
    "        return graph_point\n",
    "\n",
    "    # assumes a 50 / 50 distribution between two class values as a baseline\n",
    "    def displayROC(self, points):\n",
    "\n",
    "        canvas = np.ones((self.SCALE+1,self.SCALE+1,3), dtype=np.int32) * 255\n",
    "        #canvas[self.SCALE-graph_point[1], graph_point[0]] = 1\n",
    "        #canvas[100-10,10] = 1\n",
    "        plt.xticks(([_tick for _tick in range(0,self.SCALE,np.int32(np.round(self.SCALE/10)))]+[self.SCALE-1]),\n",
    "                   ['0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0'])\n",
    "        plt.yticks(([_tick for _tick in range(0,self.SCALE,np.int32(np.round(self.SCALE/10)))]+[self.SCALE-1]),\n",
    "                   ['1.0', '0.9', '0.8', '0.7', '0.6', '0.5', '0.4', '0.3', '0.2', '0.1', '0'])\n",
    "\n",
    "        cv2.line(canvas,(0,self.SCALE),(self.SCALE,0), (255,0,0), thickness=20)\n",
    "\n",
    "        for i in points:\n",
    "            cv2.circle(canvas,(self.SCALE - i[1], i[0]), 30, (0,0,0), -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "        plt.imshow(canvas)\n",
    "        plt.show()\n",
    "\n",
    "        # print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequentItemSets.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class FrequentItemSet:\n",
    "\n",
    "    SUPPORT = 100  # should maybe call this MIN_SUPPORT\n",
    "    DB = None\n",
    "    CONFIDENCE = 0.1\n",
    "    INTEREST = 1\n",
    "\n",
    "\n",
    "    def __init__(self, db, support, confidence):\n",
    "        self.DB = db\n",
    "        self.SUPPORT = support\n",
    "        self.CONFIDENCE = confidence\n",
    "\n",
    "    def checkAxis(self,array):\n",
    "        for i in array:\n",
    "            if i == 0:\n",
    "                return 0\n",
    "        return 1\n",
    "\n",
    "    def createPotentialSets(self, k, fis):\n",
    "        new_fis = []\n",
    "        #k = 3  # for testing\n",
    "        if k == 1:\n",
    "            potential_fis = [[i] for i in sorted(list(self.DB))]\n",
    "        else:\n",
    "            potential_fis = []\n",
    "            previous_sets = fis[-1]  # assuming we have run this previously already for frequent itemsets of size k-1\n",
    "            #print(previous_sets)\n",
    "            #print(len(previous_sets))\n",
    "            #previous_sets = [['A','B'],['A','C'],['C','D']]\n",
    "            #print(previous_sets)\n",
    "            for i in range(len(previous_sets)):\n",
    "                for j in previous_sets[(i+1):]:\n",
    "                    #print('{} - {}'.format(previous_sets[i], j))\n",
    "                    if previous_sets[i][:-1] == j[:-1]:\n",
    "                        potential_fis.append((previous_sets[i]+[j[-1]]))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        # print(potential_fis)\n",
    "        # need to validate the potential FIS by counting their support in the database\n",
    "\n",
    "        #potential_fis = [['fall']]\n",
    "        #print(potential_fis)\n",
    "        print('\\nK = {}'.format(k))\n",
    "        print('Pruning Step:   Start - {} potential sets'.format(len(potential_fis)))\n",
    "\n",
    "        # this is the pruning step to make sure that all itemsets are indeed frequent\n",
    "        for i in potential_fis:\n",
    "            # print(i)  # prints the potential frequent item set\n",
    "            item_set = self.DB[i]\n",
    "            length = len(i)\n",
    "            #print(length)\n",
    "            temp = np.sum(item_set, axis=1)\n",
    "            #new = np.array(temp).reshape(-1,)\n",
    "            support = np.array(np.where(temp == length)).size\n",
    "\n",
    "            # Below works but is really slow\n",
    "            # support = np.sum(np.apply_along_axis(self.checkAxis, 1, item_set))\n",
    "            if support >= self.SUPPORT:\n",
    "                new_fis.append(i)\n",
    "            #print(support)\n",
    "            #break\n",
    "        print('Pruning Step:   Complete - {} sets remaining'.format(len(new_fis)))\n",
    "        #print(new_fis)\n",
    "        return new_fis\n",
    "\n",
    "        #\n",
    "\n",
    "    def generateFIS(self):\n",
    "        k = 2\n",
    "        fis = []\n",
    "        current_fis = self.createPotentialSets(1, fis)\n",
    "        fis.append(current_fis)\n",
    "        #print('k: 1  length: {}'.format(len(current_fis)))\n",
    "        while len(current_fis) > 0:\n",
    "            current_fis = []\n",
    "            current_fis = self.createPotentialSets(k, fis)\n",
    "            k += 1\n",
    "            if len(current_fis) > 0:\n",
    "                fis.append(current_fis)\n",
    "            #break # remove after development\n",
    "            #print('k: {}  length: {}'.format(k, len(current_fis)))\n",
    "\n",
    "        return fis\n",
    "\n",
    "    def generateClosedSets(self, all_sets):\n",
    "        closed = {}\n",
    "        all_sets.reverse()\n",
    "        for i in range(len(all_sets)):\n",
    "            for j in all_sets[i]:\n",
    "                add = True\n",
    "                if i == 0:\n",
    "                    item_set = self.DB[j]\n",
    "                    length = len(j)\n",
    "                    # print(length)\n",
    "                    temp = np.sum(item_set, axis=1)\n",
    "                    # new = np.array(temp).reshape(-1,)\n",
    "                    support = np.array(np.where(temp == length)).size\n",
    "                    closed[frozenset(j)] = support\n",
    "                else:\n",
    "                    item_set = self.DB[j]\n",
    "                    length = len(j)\n",
    "                    # print(length)\n",
    "                    temp = np.sum(item_set, axis=1)\n",
    "                    # new = np.array(temp).reshape(-1,)\n",
    "                    support = np.array(np.where(temp == length)).size\n",
    "                    for k in all_sets[i-1]:\n",
    "                        if set(j).issubset(set(k)):\n",
    "                            # compute support for k\n",
    "                            item_set2 = self.DB[k]\n",
    "                            length2 = len(k)\n",
    "                            # print(length)\n",
    "                            temp2 = np.sum(item_set2, axis=1)\n",
    "                            # new = np.array(temp).reshape(-1,)\n",
    "                            support2 = np.array(np.where(temp2 == length2)).size\n",
    "\n",
    "                            if support2 >= support:\n",
    "                                add = False\n",
    "                                break\n",
    "                            else:\n",
    "                                pass\n",
    "                    if add:\n",
    "                        closed[frozenset(j)] = support\n",
    "        return closed\n",
    "\n",
    "    def deriveInterest(self, fis):\n",
    "        rules = []\n",
    "        rules2 = []\n",
    "        rules3 = []\n",
    "        all_length = len(self.DB[:])\n",
    "        for i in fis:\n",
    "            entry = list(i)\n",
    "            temp = entry.copy()\n",
    "            for j in entry:\n",
    "                prob_y = np.sum(self.DB[j]) / all_length\n",
    "                prob_entry = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "                temp.remove(j)\n",
    "                prob_x = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "                confidence = prob_entry / prob_x\n",
    "                # print(confidence)\n",
    "                if (prob_y < confidence) and (confidence >= self.CONFIDENCE):  # filters all of the trivial rules\n",
    "                    interest = prob_entry/(prob_x * prob_y)\n",
    "                    if interest > 1:\n",
    "                        rules.append('{} --> {} : interest={:05.3f}     confidence={:05.4f}'.format(temp, [j],\n",
    "                                                                                             interest, confidence))\n",
    "                        rules2.append([temp,[j]])\n",
    "                        \n",
    "        # The following works, but is inefficient and takes too long to finish when minimum support is low\n",
    "        # this loop merges appropriate rules to create longer ones\n",
    "        # for i in range(len(rules2)):\n",
    "        #     for j in rules2[i:]:\n",
    "        #         if len(set(j[1]) & set(rules2[i][1])) != len(j[1]):\n",
    "        #             entry = list(set(rules2[i][0] + j[0] + rules2[i][1] + j[1]))\n",
    "        #             temp = entry.copy()\n",
    "        #             prob_entry = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "        #             temp.remove(j[1][0])\n",
    "        #             temp.remove(rules2[i][1][0])\n",
    "        #             if len(temp) != 0:\n",
    "        #                 y = list(set(rules2[i][1] + j[1]))\n",
    "        #                 prob_x = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "        #                 prob_y = np.array(np.where(np.sum(self.DB[y], axis=1) == len(y))).size / all_length\n",
    "        #                 confidence = prob_entry / prob_x\n",
    "        #                 #print(confidence)\n",
    "        #                 #print(prob_y)\n",
    "        #                 if (prob_y <= confidence) and (\n",
    "        #                         confidence >= self.CONFIDENCE):  # filters all of the trivial rules\n",
    "        #                     try:\n",
    "        #                         interest = prob_entry / (prob_x * prob_y)\n",
    "        #                     except ZeroDivisionError:\n",
    "        #                         interest = 0\n",
    "        #                     #print(interest)\n",
    "        #                     if interest > 1:  # change back to 1\n",
    "        #                         rules.append('{} --> {} : interest={:05.3f}     confidence={:05.4f}'.format(temp, y,\n",
    "        #                                                                                                     interest,\n",
    "        #                                                                                                     confidence))\n",
    "        #                         rules3.append([temp, j])\n",
    "        #     if i == (len(rules2) - 1):\n",
    "        #         rules2 = rules3.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return rules\n",
    "\n",
    "\n",
    "\n",
    "    def getSupport(self):\n",
    "        return self.SUPPORT\n",
    "\n",
    "    def getFIS(self):\n",
    "        return self.FIS\n",
    "\n",
    "def main():\n",
    "\n",
    "    min_support = 20  # ran with value 10\n",
    "    min_confidence = 0.5\n",
    "    min_interest = 1.0\n",
    "    df = pd.read_csv('Structured Data/epi_r.csv')  # read in data\n",
    "\n",
    "    # PREPROCESSING STAGE ---------------------------------------------------------------\n",
    "\n",
    "    df = df.drop(['title', 'rating', 'calories', 'protein', 'fat', 'sodium'], axis=1)\n",
    "\n",
    "    # After running once decided to drop these as they seemed to lead to uninteresting results.\n",
    "    # They alos had quite a high occurences which leads to explosions in complexity when looking for frequent item sets\n",
    "    # with smaller support values.  Also many are generalization of much more specific information also included\n",
    "    # therefore many times they lead to redundant information\n",
    "    df = df.drop(\n",
    "        ['bon appétit', 'kosher', 'peanut free', 'pescatarian', 'side', 'soy free', 'tree nut free', 'vegetarian',\n",
    "         'wheat/gluten-free', 'no sugar added', 'dairy free', 'vegan', 'quick & easy', 'gourmet', 'bake',\n",
    "         'kidney friendly', 'fruit', 'vegetable', 'dessert', 'alcoholic','backyard bbq', 'fourth of july', 'grill',\n",
    "         'grill/barbecue', 'summer', 'healthy', 'dinner', 'appetizer', 'chill', 'christmas', 'cocktail party', 'drink',\n",
    "         'fall', 'herb', 'high fiber', 'kid-friendly', 'lemon', 'low cal', 'lunch', 'no-cook', 'roast', 'salad',\n",
    "         'sauce', 'sauté', 'soup/stew', 'spring', 'sugar conscious', 'thanksgiving', 'winter', 'brunch', 'breakfast',\n",
    "         'berry', 'nut', 'bon app��tit'], axis=1)\n",
    "    # df = df.drop(\n",
    "    #     ['bon appétit', 'kosher', 'peanut free', 'pescatarian', 'side', 'soy free', 'tree nut free', 'vegetarian',\n",
    "    #      'wheat/gluten-free', 'no sugar added', 'dairy free', 'vegan', 'quick & easy', 'gourmet', 'bake',\n",
    "    #      'kidney friendly', 'fruit', 'vegetable', 'dessert', 'alcoholic','backyard bbq', 'fourth of july', 'grill',\n",
    "    #      'grill/barbecue', 'summer', 'healthy', 'dinner', 'appetizer', 'chill', 'christmas', 'cocktail party', 'drink',\n",
    "    #      'fall', 'herb', 'high fiber', 'kid-friendly', 'lemon', 'low cal', 'lunch', 'no-cook', 'roast', 'salad',\n",
    "    #      'sauce', 'sauté', 'soup/stew', 'spring', 'sugar conscious', 'thanksgiving', 'winter', 'brunch', 'breakfast',\n",
    "    #      'berry', 'nut'], axis=1)\n",
    "    df = df.to_sparse(fill_value=0)\n",
    "    fis = FrequentItemSet(df,min_support,min_confidence)\n",
    "    #fis.generateFIS()\n",
    "    print('FREQUENT ITEM SET ANALYSIS')\n",
    "    print('Items: {}'.format(np.array(df[:]).shape[1]))\n",
    "    print('Rows: {}'.format(np.array(df[:]).shape[0]))\n",
    "    print('Min Support: {}'.format(min_support))\n",
    "    print('Min Confidence: {}'.format(min_confidence))\n",
    "    print('Min Interestingness: {}'.format(min_interest))\n",
    "    all_item_sets = fis.generateFIS()\n",
    "    count = np.sum(np.array([len(i) for i in all_item_sets]))\n",
    "    print('\\nGenerated {} Frequent Item Sets'.format(count))\n",
    "    closed_item_sets = fis.generateClosedSets(all_item_sets)\n",
    "    print('\\nCLOSED ITEM SETS:')\n",
    "    print('Number of sets: {}\\n'.format(len(closed_item_sets)))\n",
    "    for i in closed_item_sets:\n",
    "        print('{}\\t-\\t{}'.format(list(i),closed_item_sets[i]))\n",
    "\n",
    "    #want to derive interestingness from the data\n",
    "\n",
    "\n",
    "    interesting_rules = fis.deriveInterest(closed_item_sets)\n",
    "    print('\\nRULES:')\n",
    "    print('Number of rules: {}\\n'.format(len(interesting_rules)))\n",
    "    for i in interesting_rules:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
