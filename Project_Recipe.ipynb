{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h1 style=\"text-align:center; color:rgb(153, 99, 0)\">EXPLORING PATTERNS AND TRENDS<br> IN EVERYDAY RECIPES</h1>\n",
    "<h2 style=\"text-align:center; color:rgb(153, 99, 0)\">SENG 474 – DATA MINING</h2>\n",
    "<br>\n",
    "<p style=\"text-align:center; font-size:14pt; font-style:italics\">November 30, 2018</p>\n",
    "<br>\n",
    "\n",
    "<div style=\"height: auto; width: 230px; text-align:right\">\n",
    "<h5 style=\"text-align: left\"><b>Prepared For:</b></h5>\n",
    "Alex Thomo<br>\n",
    "<h5 style=\"text-align: left\"><b>By:</b></h5>\n",
    "Abdulla Almahmood(V00857493)<br>\n",
    "Max Gunton (V00511318)<br>\n",
    "Yaxi Yu(V00828218)<br>\n",
    "</div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.4em;text-decoration:underline;color:orange\">**Introduction**</span>\n",
    "<p>As long as there has been life on earth there has been food, and no other animal has quite mastered the art of combining and preparing food quite as much as humans.  </p>\n",
    "<p>Scientists generally agree that our early ancestors, Homo erectus, first appeared in Africa 1–2 million years ago. They spread throughout the world and evolved into ancient humans, and approximately 250,000 years ago: hearths appeared.  This is by most accounts the accepted archeological estimate for invention of cooking.  </p>\n",
    "<p>Knowing that cooking and development of recipes has been on going for 250,000 years we figured there must be hidden patterns yet to be discovered in the ingredients and the cooking techniques used.  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:1.4em; text-decoration:underline\">**1.0 Data Collection**</span>\n",
    "<h4 style=\"color:orange\">You are only as good as your data!  And our data was …. Ok.  </h4>\n",
    "<p>Initially we had hoped to collect and process our own data, but we were all quite busy this semester.  Therefore, we decided that in order to get the most out of applying the algorithms and techniques we learned, we would simply use an already existing dataset.  </p>\n",
    "<p>The dataset we chose to use was uploaded to Kaggle.com and can be found using the following link:</p>\n",
    "\t&nbsp;<a href=\"https://www.kaggle.com/hugodarwood/epirecipes#full_format_recipes.json\">https://www.kaggle.com/hugodarwood/epirecipes#full_format_recipes.json</a>\n",
    "<p>The data comes in the form of a 26.7 CSV file and is composed of 20052 rows and 680 columns.  Each row represents a recipe structured as follows:</p>\n",
    "<p>title | rating | calories | protein\tfat\tsodium | … | \"characteristics & ingredients\" </p>\n",
    "\n",
    "<p>Where the first 6 row provide information about each recipe and the following 674 rows contain a 0 or a 1 depending on whether they contain the corresponding ingredient or satisfy the characteristic.  </p>\n",
    "\n",
    "<h3 style=\"color:orange\">1.1 Pitfalls of Our Dataset</h3>\n",
    "\n",
    "<h4 style=\"color:orange\">Overly Simplified</h4>\n",
    "<p>One of the major downsides of using this provided dataset is that you can’t force anything out of it that isn’t already there.  For example, the presence of an ingredient is a good start when it comes to classifying recipes, but it is only part of the equation.  And we would be foolish to believe that the ratios of ingredients (their normalized weights) don’t also play a major part in finding patterns.  If we were collecting the data ourselves, we would have liked to include the weights of the ingredients instead of simply a 1 or 0.  </p>\n",
    "\n",
    "\n",
    "<h4 style=\"color:orange\">Flavour Molecule</h4>\n",
    "<p>Given more time, we would have also liked to have added information about the composition of the ingredients themselves.  More specifically, the composition of flavour molecules that make up each ingredient.  We believe that this is the direction that would result in the most interesting results.  Using these we could compute the Cosine similarity between ingredients which would allow us to offer recommendation for food pairings.  As well as discover which flavor molecules go well with others.  However, gathering the data and structuring it proved to be too time consuming for the scope of this project.  It would also have added a level of complexity that may have been more than we could handle as beginners to data mining.  </p>\n",
    "\n",
    "<h3 style=\"color:orange\">1.2 Benefits of Our Dataset</h3>\n",
    "<p>Some of the positive things about the dataset we are using is that it contains many attributes/columns, and this means that there are many potential relationships to be discovered.  In addition, the data came ready to go for frequent item set analysis, which allowed us to focus on implementing and optimizing the algorithm, rather than playing around with the data.  </p>\n",
    "<p>Our dataset is also large, and this meant that we had to keep in mind efficiency as well as implement things in such a way that they could be generalized.  It also gave us a taste of how things go in industry, when you can’t see all your data at once.  This really drove the importance of using the visualization tools in the matplotlib library.  And although we were able to load our entire dataset into excel, it was on the upper limit of what excel could handle.  This meant that any data preprocessing had to be done using Python, and in a generalized way as it wasn’t feasible to do it by hand.  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:1.4em; text-decoration:underline\">**2.0 Data Preprocessing**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing process aims to clean and transform the raw data to an useful form. In\n",
    "order to do that, we follow the next steps:  \n",
    "2.1. Deal with missing values  \n",
    "2.2. Data normalization  \n",
    "2.3. Data visualization  \n",
    "2.4. Training and Testing data set generation: To generate the files required for the\n",
    "data mining process (i.e, ARFF files for the WEKA tool)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Structured Data/epi_r.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.1. Deal With Missing Values.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at how we can identify and mark values as missing.  \n",
    "We can use plots and summary statistics to help identify missing or corrupt data.  \n",
    "We can load the dataset as a Pandas DataFrame and print summary statistics on each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             rating      calories        protein           fat        sodium  \\\n",
      "count  20052.000000  1.593500e+04   15890.000000  1.586900e+04  1.593300e+04   \n",
      "mean       3.714467  6.322958e+03     100.160793  3.468775e+02  6.225975e+03   \n",
      "std        1.340829  3.590460e+05    3840.318527  2.045611e+04  3.333182e+05   \n",
      "min        0.000000  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
      "25%        3.750000  1.980000e+02       3.000000  7.000000e+00  8.000000e+01   \n",
      "50%        4.375000  3.310000e+02       8.000000  1.700000e+01  2.940000e+02   \n",
      "75%        4.375000  5.860000e+02      27.000000  3.300000e+01  7.110000e+02   \n",
      "max        5.000000  3.011122e+07  236489.000000  1.722763e+06  2.767511e+07   \n",
      "\n",
      "          #cakeweek    #wasteless  22-minute meals  3-ingredient recipes  \\\n",
      "count  20052.000000  20052.000000     20052.000000          20052.000000   \n",
      "mean       0.000299      0.000050         0.000848              0.001346   \n",
      "std        0.017296      0.007062         0.029105              0.036671   \n",
      "min        0.000000      0.000000         0.000000              0.000000   \n",
      "25%        0.000000      0.000000         0.000000              0.000000   \n",
      "50%        0.000000      0.000000         0.000000              0.000000   \n",
      "75%        0.000000      0.000000         0.000000              0.000000   \n",
      "max        1.000000      1.000000         1.000000              1.000000   \n",
      "\n",
      "       30 days of groceries      ...       yellow squash        yogurt  \\\n",
      "count          20052.000000      ...        20052.000000  20052.000000   \n",
      "mean               0.000349      ...            0.001247      0.026332   \n",
      "std                0.018681      ...            0.035288      0.160123   \n",
      "min                0.000000      ...            0.000000      0.000000   \n",
      "25%                0.000000      ...            0.000000      0.000000   \n",
      "50%                0.000000      ...            0.000000      0.000000   \n",
      "75%                0.000000      ...            0.000000      0.000000   \n",
      "max                1.000000      ...            1.000000      1.000000   \n",
      "\n",
      "            yonkers          yuca      zucchini     cookbooks     leftovers  \\\n",
      "count  20052.000000  20052.000000  20052.000000  20052.000000  20052.000000   \n",
      "mean       0.000050      0.000299      0.014861      0.000150      0.000349   \n",
      "std        0.007062      0.017296      0.121001      0.012231      0.018681   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "              snack    snack week        turkey  \n",
      "count  20052.000000  20052.000000  20052.000000  \n",
      "mean       0.001396      0.000948      0.022741  \n",
      "std        0.037343      0.030768      0.149080  \n",
      "min        0.000000      0.000000      0.000000  \n",
      "25%        0.000000      0.000000      0.000000  \n",
      "50%        0.000000      0.000000      0.000000  \n",
      "75%        0.000000      0.000000      0.000000  \n",
      "max        1.000000      1.000000      1.000000  \n",
      "\n",
      "[8 rows x 679 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we chain a .sum() method on the dataframe, we can see which column contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                       0\n",
       "rating                      0\n",
       "calories                 4117\n",
       "protein                  4162\n",
       "fat                      4183\n",
       "sodium                   4119\n",
       "#cakeweek                   0\n",
       "#wasteless                  0\n",
       "22-minute meals             0\n",
       "3-ingredient recipes        0\n",
       "30 days of groceries        0\n",
       "advance prep required       0\n",
       "alabama                     0\n",
       "alaska                      0\n",
       "alcoholic                   0\n",
       "almond                      0\n",
       "amaretto                    0\n",
       "anchovy                     0\n",
       "anise                       0\n",
       "anniversary                 0\n",
       "anthony bourdain            0\n",
       "aperitif                    0\n",
       "appetizer                   0\n",
       "apple                       0\n",
       "apple juice                 0\n",
       "apricot                     0\n",
       "arizona                     0\n",
       "artichoke                   0\n",
       "arugula                     0\n",
       "asian pear                  0\n",
       "                         ... \n",
       "walnut                      0\n",
       "wasabi                      0\n",
       "washington                  0\n",
       "washington, d.c.            0\n",
       "watercress                  0\n",
       "watermelon                  0\n",
       "wedding                     0\n",
       "weelicious                  0\n",
       "west virginia               0\n",
       "westwood                    0\n",
       "wheat/gluten-free           0\n",
       "whiskey                     0\n",
       "white wine                  0\n",
       "whole wheat                 0\n",
       "wild rice                   0\n",
       "windsor                     0\n",
       "wine                        0\n",
       "winter                      0\n",
       "wisconsin                   0\n",
       "wok                         0\n",
       "yellow squash               0\n",
       "yogurt                      0\n",
       "yonkers                     0\n",
       "yuca                        0\n",
       "zucchini                    0\n",
       "cookbooks                   0\n",
       "leftovers                   0\n",
       "snack                       0\n",
       "snack week                  0\n",
       "turkey                      0\n",
       "Length: 680, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the column names which contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calories', 'protein', 'fat', 'sodium']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest strategy for handling missing data is to remove records that contain a missing value.  \n",
    "We can do this by creating a new Pandas DataFrame with the rows containing missing values removed.  \n",
    "Pandas provides the dropna() function that can be used to drop either columns or rows with missing data. We can use dropna() to remove all rows with missing data, as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again to see if we still have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.2. Data normalization.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>#cakeweek</th>\n",
       "      <th>#wasteless</th>\n",
       "      <th>22-minute meals</th>\n",
       "      <th>3-ingredient recipes</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow squash</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>yonkers</th>\n",
       "      <th>yuca</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>cookbooks</th>\n",
       "      <th>leftovers</th>\n",
       "      <th>snack</th>\n",
       "      <th>snack week</th>\n",
       "      <th>turkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lentil, Apple, and Turkey Wrap</td>\n",
       "      <td>2.500</td>\n",
       "      <td>426.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n",
       "      <td>4.375</td>\n",
       "      <td>403.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potato and Fennel Soup Hodge</td>\n",
       "      <td>3.750</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spinach Noodle Casserole</td>\n",
       "      <td>3.125</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Best Blts</td>\n",
       "      <td>4.375</td>\n",
       "      <td>948.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  rating  calories  protein  \\\n",
       "0              Lentil, Apple, and Turkey Wrap    2.500     426.0     30.0   \n",
       "1  Boudin Blanc Terrine with Red Onion Confit    4.375     403.0     18.0   \n",
       "2                Potato and Fennel Soup Hodge    3.750     165.0      6.0   \n",
       "4                    Spinach Noodle Casserole    3.125     547.0     20.0   \n",
       "5                               The Best Blts    4.375     948.0     19.0   \n",
       "\n",
       "    fat  sodium  #cakeweek  #wasteless  22-minute meals  3-ingredient recipes  \\\n",
       "0   7.0   559.0          0           0                0                     0   \n",
       "1  23.0  1439.0          0           0                0                     0   \n",
       "2   7.0   165.0          0           0                0                     0   \n",
       "4  32.0   452.0          0           0                0                     0   \n",
       "5  79.0  1042.0          0           0                0                     0   \n",
       "\n",
       "    ...    yellow squash  yogurt  yonkers  yuca  zucchini  cookbooks  \\\n",
       "0   ...                0       0        0     0         0          0   \n",
       "1   ...                0       0        0     0         0          0   \n",
       "2   ...                0       0        0     0         0          0   \n",
       "4   ...                0       0        0     0         0          0   \n",
       "5   ...                0       0        0     0         0          0   \n",
       "\n",
       "   leftovers  snack  snack week  turkey  \n",
       "0          0      0           0       1  \n",
       "1          0      0           0       0  \n",
       "2          0      0           0       0  \n",
       "4          0      0           0       0  \n",
       "5          0      0           0       0  \n",
       "\n",
       "[5 rows x 680 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, the 1st column stores the name of each recipe;  \n",
    "the column 2 to 6 store the generational info of each recipe;  \n",
    "and the rest store the ingredients respectively (the value of these columns are eigher 0 or 1, showing whether each ingredient exist in this recipe or not).   \n",
    "\n",
    "We want to normalize the data in the 2nd to the 6th columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.500</td>\n",
       "      <td>426.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>559.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.375</td>\n",
       "      <td>403.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.750</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.125</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.375</td>\n",
       "      <td>948.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1042.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  calories  protein   fat  sodium\n",
       "0   2.500     426.0     30.0   7.0   559.0\n",
       "1   4.375     403.0     18.0  23.0  1439.0\n",
       "2   3.750     165.0      6.0   7.0   165.0\n",
       "4   3.125     547.0     20.0  32.0   452.0\n",
       "5   4.375     948.0     19.0  79.0  1042.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_general_info = data[['rating', 'calories', 'protein', 'fat', 'sodium']]\n",
    "data_general_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a method to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    return (df - df.min()) * 1.0 / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this method to normalze these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  calories   protein       fat    sodium\n",
       "0   0.500  0.000014  0.000127  0.000004  0.000020\n",
       "1   0.875  0.000013  0.000076  0.000013  0.000052\n",
       "2   0.750  0.000005  0.000025  0.000004  0.000006\n",
       "4   0.625  0.000018  0.000085  0.000019  0.000016\n",
       "5   0.875  0.000031  0.000080  0.000046  0.000038"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_general_info_normalized = data_general_info.apply(normalize)\n",
    "data_general_info_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**2.3. Data Visualization.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use seaborn (a Python data visualization library based on matplotlib) to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data = data_general_info_normalized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A violin plot is similar to a box plot with a rotated kernel density plot on each side.  \n",
    "Use Violin Plot to visualise the distribution of the data and its probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalma\\Anaconda33\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot(data=data_general_info_normalized)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Global attribute names:  \n",
    "    **data** ... full dataset without missing values  \n",
    "    **data_general_info** ... 2nd to 6th columns without missing values  \n",
    "    **data_general_info_normalized**  ... normalized 2nd to 6th columns without missing values\n",
    "</span>  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;\">**2.4 Training and Testing data set generation**</span>  \n",
    "We decided to implement the training / testing split using K-Fold cross validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:1.4em; text-decoration:underline\">**3. Data Mining**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. This project is aiming to find interesting patterns and relationships between vairous attributes and do predictions on the ratings of each recipe. \n",
    "\n",
    "To achieve this, we first tried to use WEKA to classify the attributes using various machine learning classifiers. However, we had a hart time importing our large csv file to Weka.\n",
    "\n",
    "We first tried to import the csv file directly to WEKA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka1.png\" style=\"height:250px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, an error occured saying \"attribute names are not unique!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka2.png\" style=\"height:100px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 680 columns in total, finding the duplicates and rename them would be time consuming. Therefore, we tried to simply remove the titles, and add a range of numbers to place them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka3.png\" style=\"height:200px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately, a different error occured when we tried to import this file again, saying \"680 Problem encountered on line: 23\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/weka4.png\" style=\"height:100px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While trying to find the resolvent online, we found that there's a number of programmers having problems loading large size csv to WEKA. Either the file itself is not accepted, or the loading and processing process take too much of their time.  \n",
    "Therefore, we come into aggreement to code the classifers ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a class of confidence interval**  \n",
    "It takes in the predictions as an array and the correct classifications and uses the two to return the confidence interval  \n",
    "We can initialize the confidence interval object and call its method later to calculate and display prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidenceInterval.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# TODO: expand evaluator to handle continuous class labels\n",
    "\n",
    "# class_labels must be discrete and not continuous and numerical\n",
    "\n",
    "class confidenceInterval:\n",
    "    CLT_PROB_DIST = {0.1 : 3.09,    # 99.8% confidence\n",
    "                 0.5 : 2.58,     # 99%     \"\n",
    "                 1.0 : 2.33,    # 98%      \"\n",
    "                 5.0 : 1.65,    # 90%      \"\n",
    "                 10.0 : 1.28,   # 80%      \"\n",
    "                 20.0 : 0.84,   # 60%      \"\n",
    "                 40.0 : 0.25}   # 20%      \"\n",
    "    MAX_SPREAD = 100\n",
    "    p = None\n",
    "    q = None\n",
    "    variance = None\n",
    "    SN = None\n",
    "    N = None\n",
    "    confidence = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, predictions=None, class_labels=None):\n",
    "        pass\n",
    "\n",
    "    def quantizeConfidence(self, confidence):\n",
    "        index = (100 - confidence)/2.0\n",
    "        values = np.array([0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 40.0])\n",
    "        diffs = abs(values - index)\n",
    "        index = values[np.argmin(diffs)]\n",
    "        return (100 - (2*index)) , index\n",
    "\n",
    "    \n",
    "    def establish(self, predictions, class_labels, confidence=90):\n",
    "        \n",
    "        assert (len(predictions) == len(class_labels))\n",
    "\n",
    "        self.confidence, index = self.quantizeConfidence(confidence)\n",
    "        self.N = len(predictions)\n",
    "\n",
    "        # ensure that the input arrays are of type numpy.array\n",
    "        predictions = np.array(predictions)     # predictions.shape = (N,)\n",
    "        class_labels = np.array(class_labels)   # class_labels.shape = (N,)\n",
    "\n",
    "        # check to make sure they are the same shape\n",
    "        assert (predictions.shape == class_labels.shape)\n",
    "\n",
    "        # compare each array element wise to see if they are the same\n",
    "        # given multiple class values if they are represented using integers\n",
    "        # the predictions can be subtracted from the actual class_labels and\n",
    "        # resulting zeros will indicate a successful predictions; anything else will\n",
    "        # indicate an error or incorrect prediction\n",
    "\n",
    "        results = class_labels - predictions\n",
    "        #print(results)  # prints correctly\n",
    "        #print(self.N)  # prints 32\n",
    "        \n",
    "        self.p = float(len([result for result in results if result == 0]))/float(self.N)\n",
    "        self.q = 1-self.p\n",
    "        self.variance = self.p*self.q/self.N\n",
    "        denom = np.sqrt(self.variance)\n",
    "\n",
    "        self.SN = [(self.p - self.CLT_PROB_DIST[index]*denom),\n",
    "                   (self.p + self.CLT_PROB_DIST[index]*denom)]\n",
    "\n",
    "        return\n",
    "        \n",
    "   \n",
    "    def printConfidence(self):\n",
    "        if self.SN != None:\n",
    "            # \\U+03F5 unicode for within the set\n",
    "            print('Successes: {}\\nErrors: {}\\n___________\\nTotal Instances: {}\\n\\nP: {}\\nQ: {}\\nVariance:'\n",
    "                  ' {}\\n'.format(self.p*self.N, self.q*self.N, self.N, self.p, self.q, self.variance))\n",
    "            print('With {}% confidence probability of correct classification is in the '\n",
    "                  'range {:.1f}% - {:.1f}%\\n\\n'.format(self.confidence,self.SN[0]*100,\n",
    "                                                   self.SN[1]*100))\n",
    "        else:\n",
    "            print('Confidence not yet established\\n\\n')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is an implementation of a cross validator**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossValidator.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\n",
    "\n",
    "# Linear Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Decision Tree Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "CLASS_LABEL = 'rating'\n",
    "NUM_CLASSES = 6\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "df = data\n",
    "\n",
    "# split data into attributes and class labels\n",
    "y = df[CLASS_LABEL]\n",
    "X = df[[col for col in df.columns if col != CLASS_LABEL]]\n",
    "\n",
    "# convert y to a numpy array\n",
    "y = np.array(y)\n",
    "y.reshape((-1,))\n",
    "\n",
    "# change y from being continuous values between 0-5 to simply being an integer number\n",
    "# 0, 1, 2, 3, 4, or 5\n",
    "y = pd.cut(y,NUM_CLASSES, right=False, labels=[i for i in range(NUM_CLASSES)])\n",
    "\n",
    "\n",
    "strat_k_fold = StratifiedKFold(n_splits=NUM_SPLITS)  # parameters n_folds=10, shuffle=False, random_state=None\n",
    "\n",
    "# Here we want to cast X and y as numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# want to find the class that occurs the least often and only take that many instances\n",
    "# from each class value so that we end up with an even distribution\n",
    "minimum = np.min([len([val for val in y if val == i]) for i in range(6)])\n",
    "\n",
    "classes = np.array([],dtype=np.int32).reshape(-1,)\n",
    "for i in range(NUM_CLASSES):\n",
    "    classes = np.append(classes,np.array((np.where(y == i)),dtype=np.int32).reshape(-1,)[:minimum])\n",
    "\n",
    "X = X[classes] # extract our subset from entire dataset\n",
    "y = y[classes] #              \"\n",
    "\n",
    "X = X[:, 1:]  # remove the title column from the dataset since these are unique to each entry anyway\n",
    "\n",
    "splits = strat_k_fold.split(X, y)\n",
    "#scores = np.array([], dtype=np.float32).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train and test our data on 5 classifiers:  \n",
    "1. KNN Classifier\n",
    "2. Logistic Regression\n",
    "3. Naive Bayes\n",
    "4. Decision Tree\n",
    "5. Perception\n",
    "And print out the results of each classifier of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalma\\Anaconda33\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD -  1\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 12.0\n",
      "Errors: 42.0\n",
      "___________\n",
      "Total Instances: 54\n",
      "\n",
      "P: 0.2222222222222222\n",
      "Q: 0.7777777777777778\n",
      "Variance: 0.003200731595793324\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.0% - 29.5%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 13.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 54\n",
      "\n",
      "P: 0.24074074074074073\n",
      "Q: 0.7592592592592593\n",
      "Variance: 0.0033849006757100037\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 16.6% - 31.5%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 14.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 54\n",
      "\n",
      "P: 0.25925925925925924\n",
      "Q: 0.7407407407407407\n",
      "Variance: 0.0035563684397703597\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 18.3% - 33.6%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 7.0\n",
      "Errors: 47.0\n",
      "___________\n",
      "Total Instances: 54\n",
      "\n",
      "P: 0.12962962962962962\n",
      "Q: 0.8703703703703703\n",
      "Variance: 0.0020893664583650864\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 7.1% - 18.8%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 7.0\n",
      "Errors: 47.0\n",
      "___________\n",
      "Total Instances: 54\n",
      "\n",
      "P: 0.12962962962962962\n",
      "Q: 0.8703703703703703\n",
      "Variance: 0.0020893664583650864\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 7.1% - 18.8%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 13.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 54\n",
      "\n",
      "P: 0.24074074074074073\n",
      "Q: 0.7592592592592593\n",
      "Variance: 0.0033849006757100037\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 16.6% - 31.5%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  2\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 7.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.14583333333333334\n",
      "Q: 0.8541666666666666\n",
      "Variance: 0.0025951244212962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 8.1% - 21.1%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 7.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.14583333333333334\n",
      "Q: 0.8541666666666666\n",
      "Variance: 0.0025951244212962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 8.1% - 21.1%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 10.0\n",
      "Errors: 38.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.20833333333333334\n",
      "Q: 0.7916666666666666\n",
      "Variance: 0.003436053240740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 13.3% - 28.3%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 10.0\n",
      "Errors: 38.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.20833333333333334\n",
      "Q: 0.7916666666666666\n",
      "Variance: 0.003436053240740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 13.3% - 28.3%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 10.0\n",
      "Errors: 38.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.20833333333333334\n",
      "Q: 0.7916666666666666\n",
      "Variance: 0.003436053240740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 13.3% - 28.3%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 9.0\n",
      "Errors: 39.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.1875\n",
      "Q: 0.8125\n",
      "Variance: 0.003173828125\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 11.5% - 26.0%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  3\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 17.0\n",
      "Errors: 30.999999999999996\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.3541666666666667\n",
      "Q: 0.6458333333333333\n",
      "Variance: 0.0047652633101851844\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 26.6% - 44.3%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 16.0\n",
      "Errors: 32.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.3333333333333333\n",
      "Q: 0.6666666666666667\n",
      "Variance: 0.00462962962962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 24.6% - 42.0%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 10.0\n",
      "Errors: 38.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.20833333333333334\n",
      "Q: 0.7916666666666666\n",
      "Variance: 0.003436053240740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 13.3% - 28.3%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 9.0\n",
      "Errors: 39.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.1875\n",
      "Q: 0.8125\n",
      "Variance: 0.003173828125\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 11.5% - 26.0%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  4\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 17.0\n",
      "Errors: 30.999999999999996\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.3541666666666667\n",
      "Q: 0.6458333333333333\n",
      "Variance: 0.0047652633101851844\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 26.6% - 44.3%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 13.0\n",
      "Errors: 35.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.2708333333333333\n",
      "Q: 0.7291666666666667\n",
      "Variance: 0.004114221643518518\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 18.9% - 35.3%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 9.0\n",
      "Errors: 39.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.1875\n",
      "Q: 0.8125\n",
      "Variance: 0.003173828125\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 11.5% - 26.0%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 6.0\n",
      "Errors: 42.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.125\n",
      "Q: 0.875\n",
      "Variance: 0.0022786458333333335\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 6.4% - 18.6%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 6.0\n",
      "Errors: 42.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.125\n",
      "Q: 0.875\n",
      "Variance: 0.0022786458333333335\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 6.4% - 18.6%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 6.0\n",
      "Errors: 42.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.125\n",
      "Q: 0.875\n",
      "Variance: 0.0022786458333333335\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 6.4% - 18.6%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  5\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 13.0\n",
      "Errors: 35.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.2708333333333333\n",
      "Q: 0.7291666666666667\n",
      "Variance: 0.004114221643518518\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 18.9% - 35.3%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 17.0\n",
      "Errors: 30.999999999999996\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.3541666666666667\n",
      "Q: 0.6458333333333333\n",
      "Variance: 0.0047652633101851844\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 26.6% - 44.3%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 13.0\n",
      "Errors: 35.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.2708333333333333\n",
      "Q: 0.7291666666666667\n",
      "Variance: 0.004114221643518518\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 18.9% - 35.3%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 7.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.14583333333333334\n",
      "Q: 0.8541666666666666\n",
      "Variance: 0.0025951244212962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 8.1% - 21.1%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 7.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.14583333333333334\n",
      "Q: 0.8541666666666666\n",
      "Variance: 0.0025951244212962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 8.1% - 21.1%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 11.0\n",
      "Errors: 37.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.22916666666666666\n",
      "Q: 0.7708333333333334\n",
      "Variance: 0.003680193865740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.2% - 30.7%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  6\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 12.0\n",
      "Errors: 36.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.25\n",
      "Q: 0.75\n",
      "Variance: 0.00390625\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 17.0% - 33.0%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 9.0\n",
      "Errors: 39.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.1875\n",
      "Q: 0.8125\n",
      "Variance: 0.003173828125\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 11.5% - 26.0%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 9.0\n",
      "Errors: 39.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.1875\n",
      "Q: 0.8125\n",
      "Variance: 0.003173828125\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 11.5% - 26.0%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 7.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.14583333333333334\n",
      "Q: 0.8541666666666666\n",
      "Variance: 0.0025951244212962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 8.1% - 21.1%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD -  7\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 12.0\n",
      "Errors: 36.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.25\n",
      "Q: 0.75\n",
      "Variance: 0.00390625\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 17.0% - 33.0%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 13.0\n",
      "Errors: 35.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.2708333333333333\n",
      "Q: 0.7291666666666667\n",
      "Variance: 0.004114221643518518\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 18.9% - 35.3%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 6.0\n",
      "Errors: 42.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.125\n",
      "Q: 0.875\n",
      "Variance: 0.0022786458333333335\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 6.4% - 18.6%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 6.0\n",
      "Errors: 42.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.125\n",
      "Q: 0.875\n",
      "Variance: 0.0022786458333333335\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 6.4% - 18.6%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 7.0\n",
      "Errors: 41.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.14583333333333334\n",
      "Q: 0.8541666666666666\n",
      "Variance: 0.0025951244212962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 8.1% - 21.1%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  8\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 13.0\n",
      "Errors: 35.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.2708333333333333\n",
      "Q: 0.7291666666666667\n",
      "Variance: 0.004114221643518518\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 18.9% - 35.3%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 12.0\n",
      "Errors: 36.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.25\n",
      "Q: 0.75\n",
      "Variance: 0.00390625\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 17.0% - 33.0%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 11.0\n",
      "Errors: 37.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.22916666666666666\n",
      "Q: 0.7708333333333334\n",
      "Variance: 0.003680193865740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.2% - 30.7%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 10.0\n",
      "Errors: 38.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.20833333333333334\n",
      "Q: 0.7916666666666666\n",
      "Variance: 0.003436053240740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 13.3% - 28.3%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 10.0\n",
      "Errors: 38.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.20833333333333334\n",
      "Q: 0.7916666666666666\n",
      "Variance: 0.003436053240740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 13.3% - 28.3%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  9\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 15.0\n",
      "Errors: 33.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.3125\n",
      "Q: 0.6875\n",
      "Variance: 0.004475911458333333\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 22.7% - 39.8%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 11.0\n",
      "Errors: 37.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.22916666666666666\n",
      "Q: 0.7708333333333334\n",
      "Variance: 0.003680193865740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.2% - 30.7%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 12.0\n",
      "Errors: 36.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.25\n",
      "Q: 0.75\n",
      "Variance: 0.00390625\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 17.0% - 33.0%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 11.0\n",
      "Errors: 37.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.22916666666666666\n",
      "Q: 0.7708333333333334\n",
      "Variance: 0.003680193865740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.2% - 30.7%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 11.0\n",
      "Errors: 37.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.22916666666666666\n",
      "Q: 0.7708333333333334\n",
      "Variance: 0.003680193865740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.2% - 30.7%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 8.0\n",
      "Errors: 40.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.16666666666666666\n",
      "Q: 0.8333333333333334\n",
      "Variance: 0.002893518518518519\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 9.8% - 23.6%\n",
      "\n",
      "\n",
      "\n",
      "FOLD -  10\n",
      "\n",
      "Logistic Regression Prediction\n",
      "\n",
      "Successes: 16.0\n",
      "Errors: 32.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.3333333333333333\n",
      "Q: 0.6666666666666667\n",
      "Variance: 0.00462962962962963\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 24.6% - 42.0%\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Successes: 11.0\n",
      "Errors: 37.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.22916666666666666\n",
      "Q: 0.7708333333333334\n",
      "Variance: 0.003680193865740741\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 15.2% - 30.7%\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Successes: 18.0\n",
      "Errors: 30.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.375\n",
      "Q: 0.625\n",
      "Variance: 0.0048828125\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 28.6% - 46.4%\n",
      "\n",
      "\n",
      "Perception\n",
      "\n",
      "Successes: 12.0\n",
      "Errors: 36.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.25\n",
      "Q: 0.75\n",
      "Variance: 0.00390625\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 17.0% - 33.0%\n",
      "\n",
      "\n",
      "KNN\n",
      "Successes: 12.0\n",
      "Errors: 36.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.25\n",
      "Q: 0.75\n",
      "Variance: 0.00390625\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 17.0% - 33.0%\n",
      "\n",
      "\n",
      "MLP\n",
      "Successes: 14.0\n",
      "Errors: 34.0\n",
      "___________\n",
      "Total Instances: 48\n",
      "\n",
      "P: 0.2916666666666667\n",
      "Q: 0.7083333333333333\n",
      "Variance: 0.004304108796296296\n",
      "\n",
      "With 80.0% confidence probability of correct classification is in the range 20.8% - 37.6%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for train_index, test_index in splits:\n",
    "    # print('Train Index: {}'.format(train_index))\n",
    "    # print('Test Index: {}'.format(test_index))\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # TODO: use a grid search to determine the best learning constant\n",
    "    #       tweek C and max iterations to get good convergence and speed mix\n",
    "    log_reg_m = LogisticRegression(solver='lbfgs', C=0.1, max_iter=50000, multi_class='multinomial') #max_iter=50000\n",
    "    nb = GaussianNB()\n",
    "    tree = DecisionTreeClassifier()\n",
    "    percep = Perceptron()\n",
    "\n",
    "    log_reg_m.fit(X_train, y_train)\n",
    "    nb.fit(X_train, y_train)\n",
    "    tree.fit(X_train, y_train)\n",
    "    percep.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # print(log_reg_m.score(X_train,y_train))  # validating on test data\n",
    "\n",
    "    y1 = log_reg_m.predict(X_test) \n",
    "    y2 = nb.predict(X_test)\n",
    "    y3 = tree.predict(X_test)\n",
    "    y4 = percep.predict(X_test)\n",
    "    \n",
    "    #------------------------------logistic regression-----------------------------\n",
    "    prediction = y1\n",
    "    c1 = confidenceInterval()\n",
    "    c1.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print('\\nFOLD -  {}\\n'.format(count))\n",
    "    print(\"Logistic Regression Prediction\\n\")\n",
    "    c1.printConfidence()\n",
    "    \n",
    "    #----------------------------------naive bayes----------------------------------\n",
    "    prediction = y2\n",
    "    c2 = confidenceInterval()\n",
    "    c2.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print(\"Naive Bayes\")\n",
    "    c2.printConfidence()   \n",
    "    \n",
    "    #---------------------------------decision tree---------------------------------\n",
    "    prediction = y3\n",
    "    c3 = confidenceInterval()\n",
    "    c3.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print(\"Decision Tree\")\n",
    "    c3.printConfidence()   \n",
    "    \n",
    "    #----------------------------------perception-----------------------------------\n",
    "    prediction = y4\n",
    "    c4 = confidenceInterval()\n",
    "    c4.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print(\"Perception\\n\")\n",
    "    c4.printConfidence()\n",
    "    \n",
    "    #----------------------------------KNN-----------------------------------\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    predicition = classifier.predict(X_test) \n",
    "    #print(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\n",
    "    c1 = confidenceInterval()\n",
    "    c1.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print(\"KNN\")\n",
    "    c1.printConfidence()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    #----------------------------------MLP-----------------------------------\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "    prediction = mlp.predict(X_test)\n",
    "    c1 = confidenceInterval()\n",
    "    c1.establish(prediction, y_test, 80) # want to know with 80% confidence\n",
    "    print(\"MLP\")\n",
    "    c1.printConfidence()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we got a list of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a class of ROC curve**  \n",
    "We can initialize the ROC curve object and call its method later to generate the ROC curve of mutiple classifiers we have  \n",
    "**NOTE: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a6841c61972a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ROC_curve.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mROC_curve\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# ROC_curve.py\n",
    "\n",
    "import cv2\n",
    "\n",
    "class ROC_curve:\n",
    "\n",
    "    SCALE = 4000\n",
    "    y = None\n",
    "    predictions = None\n",
    "\n",
    "    def __init__(self, predictions, y):\n",
    "        self.y = (np.array(y, dtype=np.int32)).reshape((-1,))\n",
    "        self.predictions = (np.array(predictions, dtype=np.int32)).reshape((-1,))\n",
    "        assert (y.shape == predictions.shape)\n",
    "\n",
    "    def compare_labels(self, axis):\n",
    "        # 0 - TP\n",
    "        # 1 - TN\n",
    "        # 2 - FP\n",
    "        # 3 - FN\n",
    "        diff = axis[0] - axis[1]\n",
    "        if diff == 0:\n",
    "            if axis[0] == 1:\n",
    "                # True Positive\n",
    "                return 0\n",
    "            else:\n",
    "                # True Negative\n",
    "                return 1\n",
    "        else:\n",
    "            if axis[0] == 0:\n",
    "                # False Positive\n",
    "                return 2\n",
    "            else:\n",
    "                # False Negative\n",
    "                return 3\n",
    "\n",
    "    def calcPoint(self):\n",
    "\n",
    "        # y = y.reshape((-1,))\n",
    "        # predictions = predictions.reshape((-1,))\n",
    "        values = np.column_stack((self.y, self.predictions))\n",
    "\n",
    "\n",
    "        # print(y)\n",
    "        # print(predictions)\n",
    "\n",
    "        a = np.apply_along_axis(self.compare_labels,1,values)\n",
    "        labels, counts = np.unique(a, return_counts=True)\n",
    "        labels2, counts2 = np.unique(self.y, return_counts=True)\n",
    "        # print(a)\n",
    "        # print(counts)\n",
    "        # print(counts2)\n",
    "        index = dict(zip(labels, counts))\n",
    "        index2 = dict(zip(labels2, counts2))\n",
    "        print(index2)\n",
    "        confusion_matrix = {'TruePos' : index.get(0,0),\n",
    "                            'TrueNeg' : index.get(1,0),\n",
    "                            'FalsePos' : index.get(2,0),\n",
    "                            'FalseNeg' : index.get(3,0),\n",
    "                            'Positives' : index2.get(1,0),\n",
    "                            'Negatives' : index2.get(0,0)\n",
    "                            }\n",
    "\n",
    "        true_positive_rate = confusion_matrix['TruePos'] / confusion_matrix['Positives']\n",
    "        false_positive_rate = confusion_matrix['FalsePos'] / confusion_matrix['Negatives']\n",
    "\n",
    "        graph_point = np.array([np.round(false_positive_rate,2)*self.SCALE,np.round(true_positive_rate,2)*self.SCALE], dtype=np.int32)\n",
    "        return graph_point\n",
    "\n",
    "    # assumes a 50 / 50 distribution between two class values as a baseline\n",
    "    def displayROC(self, points):\n",
    "\n",
    "        canvas = np.ones((self.SCALE+1,self.SCALE+1,3), dtype=np.int32) * 255\n",
    "        #canvas[self.SCALE-graph_point[1], graph_point[0]] = 1\n",
    "        #canvas[100-10,10] = 1\n",
    "        plt.xticks(([_tick for _tick in range(0,self.SCALE,np.int32(np.round(self.SCALE/10)))]+[self.SCALE-1]),\n",
    "                   ['0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0'])\n",
    "        plt.yticks(([_tick for _tick in range(0,self.SCALE,np.int32(np.round(self.SCALE/10)))]+[self.SCALE-1]),\n",
    "                   ['1.0', '0.9', '0.8', '0.7', '0.6', '0.5', '0.4', '0.3', '0.2', '0.1', '0'])\n",
    "\n",
    "        cv2.line(canvas,(0,self.SCALE),(self.SCALE,0), (255,0,0), thickness=20)\n",
    "\n",
    "        for i in points:\n",
    "            cv2.circle(canvas,(self.SCALE - i[1], i[0]), 30, (0,0,0), -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "        plt.imshow(canvas)\n",
    "        plt.show()\n",
    "\n",
    "        # print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequentItemSets.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class FrequentItemSet:\n",
    "\n",
    "    SUPPORT = 100  # should maybe call this MIN_SUPPORT\n",
    "    DB = None\n",
    "    CONFIDENCE = 0.1\n",
    "    INTEREST = 1\n",
    "\n",
    "\n",
    "    def __init__(self, db, support, confidence):\n",
    "        self.DB = db\n",
    "        self.SUPPORT = support\n",
    "        self.CONFIDENCE = confidence\n",
    "\n",
    "    def checkAxis(self,array):\n",
    "        for i in array:\n",
    "            if i == 0:\n",
    "                return 0\n",
    "        return 1\n",
    "\n",
    "    def createPotentialSets(self, k, fis):\n",
    "        new_fis = []\n",
    "        #k = 3  # for testing\n",
    "        if k == 1:\n",
    "            potential_fis = [[i] for i in sorted(list(self.DB))]\n",
    "        else:\n",
    "            potential_fis = []\n",
    "            previous_sets = fis[-1]  # assuming we have run this previously already for frequent itemsets of size k-1\n",
    "            #print(previous_sets)\n",
    "            #print(len(previous_sets))\n",
    "            #previous_sets = [['A','B'],['A','C'],['C','D']]\n",
    "            #print(previous_sets)\n",
    "            for i in range(len(previous_sets)):\n",
    "                for j in previous_sets[(i+1):]:\n",
    "                    #print('{} - {}'.format(previous_sets[i], j))\n",
    "                    if previous_sets[i][:-1] == j[:-1]:\n",
    "                        potential_fis.append((previous_sets[i]+[j[-1]]))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        # print(potential_fis)\n",
    "        # need to validate the potential FIS by counting their support in the database\n",
    "\n",
    "        #potential_fis = [['fall']]\n",
    "        #print(potential_fis)\n",
    "        print('\\nK = {}'.format(k))\n",
    "        print('Pruning Step:   Start - {} potential sets'.format(len(potential_fis)))\n",
    "\n",
    "        # this is the pruning step to make sure that all itemsets are indeed frequent\n",
    "        for i in potential_fis:\n",
    "            # print(i)  # prints the potential frequent item set\n",
    "            item_set = self.DB[i]\n",
    "            length = len(i)\n",
    "            #print(length)\n",
    "            temp = np.sum(item_set, axis=1)\n",
    "            #new = np.array(temp).reshape(-1,)\n",
    "            support = np.array(np.where(temp == length)).size\n",
    "\n",
    "            # Below works but is really slow\n",
    "            # support = np.sum(np.apply_along_axis(self.checkAxis, 1, item_set))\n",
    "            if support >= self.SUPPORT:\n",
    "                new_fis.append(i)\n",
    "            #print(support)\n",
    "            #break\n",
    "        print('Pruning Step:   Complete - {} sets remaining'.format(len(new_fis)))\n",
    "        #print(new_fis)\n",
    "        return new_fis\n",
    "\n",
    "        #\n",
    "\n",
    "    def generateFIS(self):\n",
    "        k = 2\n",
    "        fis = []\n",
    "        current_fis = self.createPotentialSets(1, fis)\n",
    "        fis.append(current_fis)\n",
    "        #print('k: 1  length: {}'.format(len(current_fis)))\n",
    "        while len(current_fis) > 0:\n",
    "            current_fis = []\n",
    "            current_fis = self.createPotentialSets(k, fis)\n",
    "            k += 1\n",
    "            if len(current_fis) > 0:\n",
    "                fis.append(current_fis)\n",
    "            #break # remove after development\n",
    "            #print('k: {}  length: {}'.format(k, len(current_fis)))\n",
    "\n",
    "        return fis\n",
    "\n",
    "    def generateClosedSets(self, all_sets):\n",
    "        closed = {}\n",
    "        all_sets.reverse()\n",
    "        for i in range(len(all_sets)):\n",
    "            for j in all_sets[i]:\n",
    "                add = True\n",
    "                if i == 0:\n",
    "                    item_set = self.DB[j]\n",
    "                    length = len(j)\n",
    "                    # print(length)\n",
    "                    temp = np.sum(item_set, axis=1)\n",
    "                    # new = np.array(temp).reshape(-1,)\n",
    "                    support = np.array(np.where(temp == length)).size\n",
    "                    closed[frozenset(j)] = support\n",
    "                else:\n",
    "                    item_set = self.DB[j]\n",
    "                    length = len(j)\n",
    "                    # print(length)\n",
    "                    temp = np.sum(item_set, axis=1)\n",
    "                    # new = np.array(temp).reshape(-1,)\n",
    "                    support = np.array(np.where(temp == length)).size\n",
    "                    for k in all_sets[i-1]:\n",
    "                        if set(j).issubset(set(k)):\n",
    "                            # compute support for k\n",
    "                            item_set2 = self.DB[k]\n",
    "                            length2 = len(k)\n",
    "                            # print(length)\n",
    "                            temp2 = np.sum(item_set2, axis=1)\n",
    "                            # new = np.array(temp).reshape(-1,)\n",
    "                            support2 = np.array(np.where(temp2 == length2)).size\n",
    "\n",
    "                            if support2 >= support:\n",
    "                                add = False\n",
    "                                break\n",
    "                            else:\n",
    "                                pass\n",
    "                    if add:\n",
    "                        closed[frozenset(j)] = support\n",
    "        return closed\n",
    "\n",
    "    def deriveInterest(self, fis):\n",
    "        rules = []\n",
    "        rules2 = []\n",
    "        rules3 = []\n",
    "        all_length = len(self.DB[:])\n",
    "        for i in fis:\n",
    "            entry = list(i)\n",
    "            temp = entry.copy()\n",
    "            for j in entry:\n",
    "                prob_y = np.sum(self.DB[j]) / all_length\n",
    "                prob_entry = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "                temp.remove(j)\n",
    "                prob_x = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "                confidence = prob_entry / prob_x\n",
    "                # print(confidence)\n",
    "                if (prob_y < confidence) and (confidence >= self.CONFIDENCE):  # filters all of the trivial rules\n",
    "                    interest = prob_entry/(prob_x * prob_y)\n",
    "                    if interest > 1:\n",
    "                        rules.append('{} --> {} : interest={:05.3f}     confidence={:05.4f}'.format(temp, [j],\n",
    "                                                                                             interest, confidence))\n",
    "                        rules2.append([temp,[j]])\n",
    "                        \n",
    "        # The following works, but is inefficient and takes too long to finish when minimum support is low\n",
    "        # this loop merges appropriate rules to create longer ones\n",
    "        # for i in range(len(rules2)):\n",
    "        #     for j in rules2[i:]:\n",
    "        #         if len(set(j[1]) & set(rules2[i][1])) != len(j[1]):\n",
    "        #             entry = list(set(rules2[i][0] + j[0] + rules2[i][1] + j[1]))\n",
    "        #             temp = entry.copy()\n",
    "        #             prob_entry = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "        #             temp.remove(j[1][0])\n",
    "        #             temp.remove(rules2[i][1][0])\n",
    "        #             if len(temp) != 0:\n",
    "        #                 y = list(set(rules2[i][1] + j[1]))\n",
    "        #                 prob_x = np.array(np.where(np.sum(self.DB[temp], axis=1) == len(temp))).size / all_length\n",
    "        #                 prob_y = np.array(np.where(np.sum(self.DB[y], axis=1) == len(y))).size / all_length\n",
    "        #                 confidence = prob_entry / prob_x\n",
    "        #                 #print(confidence)\n",
    "        #                 #print(prob_y)\n",
    "        #                 if (prob_y <= confidence) and (\n",
    "        #                         confidence >= self.CONFIDENCE):  # filters all of the trivial rules\n",
    "        #                     try:\n",
    "        #                         interest = prob_entry / (prob_x * prob_y)\n",
    "        #                     except ZeroDivisionError:\n",
    "        #                         interest = 0\n",
    "        #                     #print(interest)\n",
    "        #                     if interest > 1:  # change back to 1\n",
    "        #                         rules.append('{} --> {} : interest={:05.3f}     confidence={:05.4f}'.format(temp, y,\n",
    "        #                                                                                                     interest,\n",
    "        #                                                                                                     confidence))\n",
    "        #                         rules3.append([temp, j])\n",
    "        #     if i == (len(rules2) - 1):\n",
    "        #         rules2 = rules3.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return rules\n",
    "\n",
    "\n",
    "\n",
    "    def getSupport(self):\n",
    "        return self.SUPPORT\n",
    "\n",
    "    def getFIS(self):\n",
    "        return self.FIS\n",
    "\n",
    "def main():\n",
    "\n",
    "    min_support = 20  # ran with value 10\n",
    "    min_confidence = 0.5\n",
    "    min_interest = 1.0\n",
    "    df = pd.read_csv('Structured Data/epi_r.csv')  # read in data\n",
    "\n",
    "    # PREPROCESSING STAGE ---------------------------------------------------------------\n",
    "\n",
    "    df = df.drop(['title', 'rating', 'calories', 'protein', 'fat', 'sodium'], axis=1)\n",
    "\n",
    "    # After running once decided to drop these as they seemed to lead to uninteresting results.\n",
    "    # They alos had quite a high occurences which leads to explosions in complexity when looking for frequent item sets\n",
    "    # with smaller support values.  Also many are generalization of much more specific information also included\n",
    "    # therefore many times they lead to redundant information\n",
    "    df = df.drop(\n",
    "        ['bon appétit', 'kosher', 'peanut free', 'pescatarian', 'side', 'soy free', 'tree nut free', 'vegetarian',\n",
    "         'wheat/gluten-free', 'no sugar added', 'dairy free', 'vegan', 'quick & easy', 'gourmet', 'bake',\n",
    "         'kidney friendly', 'fruit', 'vegetable', 'dessert', 'alcoholic','backyard bbq', 'fourth of july', 'grill',\n",
    "         'grill/barbecue', 'summer', 'healthy', 'dinner', 'appetizer', 'chill', 'christmas', 'cocktail party', 'drink',\n",
    "         'fall', 'herb', 'high fiber', 'kid-friendly', 'lemon', 'low cal', 'lunch', 'no-cook', 'roast', 'salad',\n",
    "         'sauce', 'sauté', 'soup/stew', 'spring', 'sugar conscious', 'thanksgiving', 'winter', 'brunch', 'breakfast',\n",
    "         'berry', 'nut', 'bon app��tit'], axis=1)\n",
    "    # df = df.drop(\n",
    "    #     ['bon appétit', 'kosher', 'peanut free', 'pescatarian', 'side', 'soy free', 'tree nut free', 'vegetarian',\n",
    "    #      'wheat/gluten-free', 'no sugar added', 'dairy free', 'vegan', 'quick & easy', 'gourmet', 'bake',\n",
    "    #      'kidney friendly', 'fruit', 'vegetable', 'dessert', 'alcoholic','backyard bbq', 'fourth of july', 'grill',\n",
    "    #      'grill/barbecue', 'summer', 'healthy', 'dinner', 'appetizer', 'chill', 'christmas', 'cocktail party', 'drink',\n",
    "    #      'fall', 'herb', 'high fiber', 'kid-friendly', 'lemon', 'low cal', 'lunch', 'no-cook', 'roast', 'salad',\n",
    "    #      'sauce', 'sauté', 'soup/stew', 'spring', 'sugar conscious', 'thanksgiving', 'winter', 'brunch', 'breakfast',\n",
    "    #      'berry', 'nut'], axis=1)\n",
    "    df = df.to_sparse(fill_value=0)\n",
    "    fis = FrequentItemSet(df,min_support,min_confidence)\n",
    "    #fis.generateFIS()\n",
    "    print('FREQUENT ITEM SET ANALYSIS')\n",
    "    print('Items: {}'.format(np.array(df[:]).shape[1]))\n",
    "    print('Rows: {}'.format(np.array(df[:]).shape[0]))\n",
    "    print('Min Support: {}'.format(min_support))\n",
    "    print('Min Confidence: {}'.format(min_confidence))\n",
    "    print('Min Interestingness: {}'.format(min_interest))\n",
    "    all_item_sets = fis.generateFIS()\n",
    "    count = np.sum(np.array([len(i) for i in all_item_sets]))\n",
    "    print('\\nGenerated {} Frequent Item Sets'.format(count))\n",
    "    closed_item_sets = fis.generateClosedSets(all_item_sets)\n",
    "    print('\\nCLOSED ITEM SETS:')\n",
    "    print('Number of sets: {}\\n'.format(len(closed_item_sets)))\n",
    "    for i in closed_item_sets:\n",
    "        print('{}\\t-\\t{}'.format(list(i),closed_item_sets[i]))\n",
    "\n",
    "    #want to derive interestingness from the data\n",
    "\n",
    "\n",
    "    interesting_rules = fis.deriveInterest(closed_item_sets)\n",
    "    print('\\nRULES:')\n",
    "    print('Number of rules: {}\\n'.format(len(interesting_rules)))\n",
    "    for i in interesting_rules:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
